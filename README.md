# cyberarxiv

R-пакет для автоматического сбора, классификации и анализа научных публикаций по кибербезопасности с arXiv. 

Проект был разработан в рамках учебного курса информационно-аналитические технологии поиска угроз информационной безопасности. Мы хотели создать инструмент, который автоматически собирает статьи по кибербезопасности, классифицирует их по темам и визуализирует результаты в удобном дашборде.

## Что делает этот проект?

Проект реализует полный ETL-пайплайн:

1. **Скачивание статей** - автоматически получает публикации с arXiv по заданным критериям
2. **Сохранение сырых данных** - сохраняет исходные данные в формате `.rds` для бэкапа
3. **Классификация** - автоматически определяет тематику статей (malware, cryptography, ML security и т.д.)
4. **Хранение в БД** - сохраняет классифицированные данные в DuckDB
5. **Визуализация** - создает интерактивный дашборд с графиками и статистикой

## Быстрый старт

### Через Docker (рекомендуется)

Самый простой способ запустить проект - использовать Docker Compose:

```bash

git clone https://github.com/IT-life1/cyberarxiv.git
cd cyberarxiv

docker-compose up --build
```

После запуска дашборд будет доступен по адресу: **http://localhost:8000**

### Что происходит при запуске?

1. Собирается Docker-образ с R и всеми зависимостями
2. Запускается ETL-пайплайн:
   - Скачиваются статьи с arXiv (по умолчанию 1000 статей)
   - Данные сохраняются в `./raw-data/arxiv_papers.rds`
   - Статьи классифицируются по темам
   - Результаты сохраняются в `./data/cyberarxiv.duckdb`
3. Рендерится и запускается дашборд на порту 8000

### Настройка параметров

Можно настроить количество статей и поисковый запрос через переменные окружения в `docker-compose.yml`:

```yaml
environment:
  - MAX_RESULTS=1000    # Сколько статей скачать
  - QUERY=              # Поисковый запрос
```

## Установка как R-пакет

Если хотите использовать пакет локально в R:

```r
# Установка через remotes
remotes::install_github("IT-life1/cyberarxiv")
```

### Пример использования

```r
library(cyberarxiv)

# Скачать 100 статей
papers <- get_arxiv_papers(max_results = 100)

# Сохранить сырые данные
save_raw_data(papers)

# Загрузить и классифицировать
raw_data <- load_raw_data()
classified <- classify_data(raw_data)

# Сохранить в БД
save_publications(classified)

# Загрузить из БД
publications <- load_publications()

# Поиск по тексту
results <- search_papers(publications, query = "malware", year = 2024)

# Рендерить дашборд
render_dashboard(output_dir = "dashboard")
```

## Архитектура проекта

```
cyberarxiv/
├── R/                    # Исходный код пакета
│   ├── get_arxiv_papers.R    # Скачивание с arXiv
│   ├── raw_data.R            # Работа с сырыми данными
│   ├── classify_data.R       # Классификация статей
│   ├── save_publications.R   # Сохранение в БД
│   ├── load_publications.R   # Загрузка из БД
│   ├── dashboard.R           # Рендеринг дашборда
│   ├── db.r                  # Создание БД
│   ├── text_utils.R          # Выделение ключевых слов
│   └── serve_dashboard.R     # HTTP сервер для дашборда
├── docker/
│   ├── run_etl.R             # ETL скрипт
│   ├── run_dashboard.R       # Скрипт запуска дашборда
│   └── start.sh              # Главный скрипт запуска
├── inst/quarto/
│   └── dashboard.qmd         # Шаблон дашборда
├── Dockerfile                # Образ для контейнеризации
└── docker-compose.yml        # Конфигурация Docker Compose
```